{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "2c9d73bb-e8d5-4aeb-a928-93db9ac6241c",
        "31985af0-d58e-4d01-8432-065537022502",
        "18lnF6HxgLsO",
        "xqm-h4-orzrd",
        "DdceMqv1gOq0"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaykhoomDalal/ECE_C147_Project/blob/FinalCode/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c9d73bb-e8d5-4aeb-a928-93db9ac6241c"
      },
      "source": [
        "# Transformer for EEG"
      ],
      "id": "2c9d73bb-e8d5-4aeb-a928-93db9ac6241c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31985af0-d58e-4d01-8432-065537022502"
      },
      "source": [
        "---\n",
        "## Imports"
      ],
      "id": "31985af0-d58e-4d01-8432-065537022502"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "446258e5-7cf2-4a73-8dd7-7b7a43fdd98e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af5c3bf-b886-44d4-c376-3850134b6f25"
      },
      "source": [
        "!pip install torch numpy matplotlib"
      ],
      "id": "446258e5-7cf2-4a73-8dd7-7b7a43fdd98e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3539f61f-fe7d-4428-b074-327a883f7f6e"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "id": "3539f61f-fe7d-4428-b074-327a883f7f6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRANSFORMER ENCODER SCRATCH (model opt2)"
      ],
      "metadata": {
        "id": "9AAIbMhNE_wR"
      },
      "id": "9AAIbMhNE_wR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utils"
      ],
      "metadata": {
        "id": "18lnF6HxgLsO"
      },
      "id": "18lnF6HxgLsO"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, time, math, tqdm, random, sys, gzip\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as dist\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def enwik8(path=None, n_train=int(90e6), n_valid=int(5e6), n_test=int(5e6)):\n",
        "    \"\"\"\n",
        "    Load the enwik8 dataset from the Hutter challenge.\n",
        "\n",
        "    Adapted from https://github.com/openai/blocksparse/blob/master/examples/transformer/enwik8.py\n",
        "\n",
        "    :param path:\n",
        "    :param n_train:\n",
        "    :param n_valid:\n",
        "    :param n_test:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if path is None:\n",
        "        path = here('data/enwik8.gz')\n",
        "\n",
        "    with gzip.open(path) if path.endswith('.gz') else open(path) as file:\n",
        "        X = np.fromstring(file.read(n_train + n_valid + n_test), dtype=np.uint8)\n",
        "        trX, vaX, teX = np.split(X, [n_train, n_train + n_valid])\n",
        "        return torch.from_numpy(trX), torch.from_numpy(vaX), torch.from_numpy(teX)\n",
        "\n",
        "def sample(lnprobs, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Sample an element from a categorical distribution\n",
        "    :param lnprobs: Outcome log-probabilities\n",
        "    :param temperature: Sampling temperature. 1.0 follows the given distribution,\n",
        "        0.0 returns the maximum probability element.\n",
        "    :return: The index of the sampled element.\n",
        "    \"\"\"\n",
        "\n",
        "    if temperature == 0.0:\n",
        "        return lnprobs.argmax()\n",
        "\n",
        "    p = F.softmax(lnprobs / temperature, dim=0)\n",
        "    cd = dist.Categorical(p)\n",
        "\n",
        "    return cd.sample()\n",
        "\n",
        "def sample_sequence(model, seed, max_context, length=600, temperature=0.5, verbose=False):\n",
        "    \"\"\"\n",
        "    Sequentially samples a sequence from the model, token by token.\n",
        "\n",
        "    :param model:\n",
        "    :param seed: The sequence to start with.\n",
        "    :param length: The total number of characters to sample.\n",
        "    :param temperature: The sampling temperature.\n",
        "    :param verbose: If true, the sampled sequence is also printed as it is sampled.\n",
        "\n",
        "    :return: The sampled sequence, including the seed.\n",
        "    \"\"\"\n",
        "\n",
        "    sequence = seed.detach().clone()\n",
        "\n",
        "    if verbose: # Print the seed, surrounded by square brackets\n",
        "        print('[', end='', flush=True)\n",
        "        for c in seed:\n",
        "            print(str(chr(c)), end='', flush=True)\n",
        "        print(']', end='', flush=True)\n",
        "\n",
        "    for _ in range(length):\n",
        "\n",
        "        # Input is the tail end of the sampled sequence (as many tokens as the model can handle)\n",
        "        input = sequence[-max_context:]\n",
        "\n",
        "        # Run the current input through the model\n",
        "        output = model(input[None, :])\n",
        "\n",
        "        # Sample the next token from the probabilitys at the last position of the output.\n",
        "        c = sample(output[0, -1, :], temperature)\n",
        "\n",
        "        if verbose:\n",
        "            print(str(chr(max(32, c))), end='', flush=True)\n",
        "\n",
        "        sequence = torch.cat([sequence, c[None]], dim=0) # Append the sampled token to the sequence\n",
        "\n",
        "    print()\n",
        "    return seed\n",
        "\n",
        "def sample_batch(data, length, batch_size):\n",
        "    \"\"\"\n",
        "    Takes the data (a single sequence of tokens) and slices out a batch of subsequences to provide as input to the model.\n",
        "\n",
        "    For each input instance, it also slices out the sequence that is shifted one position to the right, to provide as a\n",
        "    target for the model.\n",
        "\n",
        "    :param data: The (training) data. A single vector of tokens represented by integers\n",
        "    :param length: The length of the subsequences in the batch.\n",
        "    :param batch_size: The number of subsequences in the batch\n",
        "    :return: A pair (input, target) of minteger matrices representing the input and target for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sample the starting indices of the sequences to slice out.\n",
        "    starts = torch.randint(size=(batch_size,), low=0, high=data.size(0) - length - 1)\n",
        "\n",
        "    # Slice out the input sequences\n",
        "    seqs_inputs  = [data[start:start + length] for start in starts]\n",
        "    # -- the start index is the one we just sampled, and the end is exactly 'lentgh' positions after that.\n",
        "    seqs_target = [data[start + 1:start + length + 1] for start in starts]\n",
        "    # -- The target is the same sequence as input, except one character ahead (we are asking the model to predict the\n",
        "    #    next character at each position)\n",
        "\n",
        "    # We now have two lists of torch vectors, which we can concatenate into matrices of batch_size-by-length\n",
        "    inputs = torch.cat([s[None, :] for s in seqs_inputs], dim=0).to(torch.long)\n",
        "    target = torch.cat([s[None, :] for s in seqs_target], dim=0).to(torch.long)\n",
        "    # -- Note that we add a singleton dimenson to each vector, s[None.,:], and then concatenate along that dimension.\n",
        "\n",
        "    return inputs, target\n",
        "\n",
        "def mask_(matrices, maskval=0.0, mask_diagonal=True):\n",
        "    \"\"\"\n",
        "    Masks out all values in the given batch of matrices where i <= j holds,\n",
        "    i < j if mask_diagonal is false\n",
        "\n",
        "    In place operation\n",
        "\n",
        "    :param tns:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "\n",
        "    h, w = matrices.size(-2), matrices.size(-1)\n",
        "\n",
        "    indices = torch.triu_indices(h, w, offset=0 if mask_diagonal else 1)\n",
        "    matrices[..., indices[0], indices[1]] = maskval\n",
        "\n",
        "def d(tensor=None):\n",
        "    \"\"\"\n",
        "    Returns a device string either for the best available device,\n",
        "    or for the device corresponding to the argument\n",
        "    :param tensor:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if tensor is None:\n",
        "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    return 'cuda' if tensor.is_cuda else 'cpu'\n",
        "\n",
        "def here(subpath=None):\n",
        "    \"\"\"\n",
        "    :return: the path in which the package resides (the directory containing the 'former' dir)\n",
        "    \"\"\"\n",
        "    if subpath is None:\n",
        "        return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))\n",
        "\n",
        "    return os.path.abspath(os.path.join(os.path.dirname(__file__), '../..', subpath))\n",
        "\n",
        "def contains_nan(tensor):\n",
        "    return bool((tensor != tensor).sum() > 0)\n",
        "\n",
        "\n",
        "tics = []\n",
        "\n",
        "\n",
        "def tic():\n",
        "    tics.append(time.time())\n",
        "\n",
        "def toc():\n",
        "    if len(tics)==0:\n",
        "        return None\n",
        "    else:\n",
        "        return time.time()-tics.pop()\n",
        "\n",
        "def slice_diag(matrix, l, dv=None):\n",
        "    \"\"\"\n",
        "    Take a batch of attention matrices for relative position encodings and slice out the relevant attentions. These\n",
        "    are the length l sequences starting at the diagonal\n",
        "\n",
        "    :param matrix:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if dv is None:\n",
        "        dv = d(matrix)\n",
        "\n",
        "    h, w = matrix.size(-2), matrix.size(-1)\n",
        "\n",
        "    assert w == 2 * l -1, f'(h, w)= {(h, w)}, l={l}'\n",
        "\n",
        "    rest = matrix.size()[:-2]\n",
        "\n",
        "    matrix = matrix.view(-1, h, w)\n",
        "    b, h, w = matrix.size()\n",
        "\n",
        "    result = matrix.view(b, -1)\n",
        "    result = torch.cat([result, torch.zeros(b, l, device=dv)], dim=1)\n",
        "    assert result.size() == (b, 2 * l * l), f'result.size() {result.size()}'\n",
        "\n",
        "    result = result.view(b, l, 2*l)\n",
        "    result = result[:, :, :l]\n",
        "\n",
        "    result = result.view(*rest, h, l)\n",
        "    return result\n",
        "\n",
        "# Used for converting between nats and bits\n",
        "LOG2E = math.log2(math.e)\n",
        "LOGE2 = math.log(2.0)\n",
        "\n",
        "def compute_compression(model, data, context, batch_size, verbose=False,\n",
        "                        tbw:SummaryWriter=None, tok=None, skip=0):\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Compute the _compression_ of a dataset under a model. That is, given a model, in how many bits could we represent\n",
        "    the dataset. This requires us to turn a given probability distribution into a code for the outcomes.\n",
        "\n",
        "    See [this video](https://youtu.be/mSneVjDvzNQ) for an explanation.\n",
        "\n",
        "    :param model: A sequence-to-sequence model that takes as input a (sub) sequence of integers and produces a probability\n",
        "    distributuion on the output.\n",
        "    :param data: A singe list of integers representing the  data\n",
        "    :return: The result of the computation in \"bits per byte\". That is, how many bits does the compressed representation\n",
        "    spend on each byte (=ASCII character) of the raw data.\n",
        "    \"\"\"\n",
        "\n",
        "    bits, tot = 0.0, 0\n",
        "    batch = []\n",
        "    # Buffer, every time it fills up, we run it through the model\n",
        "    # --- For the sake of speed we want to process the data in batches. For each token in the data, we make a\n",
        "    #     prediction based on all the `context` tokens before it. This means that for each subsequence in the batch, we\n",
        "    #     need to shift the start/end indices ahead by one token.\n",
        "    #\n",
        "    #     After we pass the batch through the model, we look at only the probabilities predicted for the last token.\n",
        "\n",
        "    target_indices = []\n",
        "    i, ic = 0, 0\n",
        "\n",
        "    for current in tqdm.trange(skip, data.size(0)) if verbose else range(skip, data.size(0)):\n",
        "\n",
        "        # `current` is the character which we will ultimately predict\n",
        "\n",
        "        fr = max(0, current - context)\n",
        "        to = current + 1\n",
        "\n",
        "        instance = data[fr:to].to(torch.long) # the subsequence of the data to add to the batch\n",
        "        # -- slice out an instance of size context + 1 (or shorter at the start of the data)\n",
        "\n",
        "        # if tok is not None:\n",
        "        #     print(instance[:-1], tok.decode(instance[:-1]))\n",
        "        #     print(instance[-1:], tok.decode(instance[-1:]))\n",
        "\n",
        "        target_indices.append(instance.size(0) - 2) # index of the last element of the input to the model\n",
        "\n",
        "        if instance.size(0) < context + 1:\n",
        "            assert skip < context # We shouldn't get here if we skip the first `context` characters\n",
        "\n",
        "            # the index in the output tensor of the character we want to predict\n",
        "            # -- It's context + 1, because we clip off the last token as a target\n",
        "\n",
        "            pad = torch.zeros(size=(context + 1 - instance.size(0),), dtype=torch.long)\n",
        "            instance = torch.cat([instance, pad], dim=0)\n",
        "            # -- the first tokens don't have enough tokens preceding them, so we pad them to the right size.\n",
        "\n",
        "            assert instance.size(0) == context + 1 # all instances should be `context` + 1 long\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            instance = instance.cuda()\n",
        "\n",
        "        batch.append(instance[None, :])\n",
        "        # -- We add a singleton dimension to concatenate along later.\n",
        "\n",
        "        if len(batch) == batch_size or current == data.size(0) - 1:\n",
        "            # batch is full or we are at the last instance, run it through the model\n",
        "\n",
        "            b = len(batch)\n",
        "\n",
        "            ti = torch.tensor(target_indices) + 1\n",
        "            all = torch.cat(batch, dim=0)\n",
        "            inputs = all[:, :-1] # input\n",
        "            target = all[torch.arange(b), ti]  # target values\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if next(model.parameters()).is_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                output = model(inputs)\n",
        "\n",
        "            if type(output) != torch.Tensor:\n",
        "                output = torch.log_softmax(output.logits, dim=2) # To make the method work for GPT2 models from Huggingface\n",
        "\n",
        "            assert output.size()[:2] == (b, context), f'was: {output.size()}, should be {(b, context, -1)}'\n",
        "\n",
        "            lnprobs = output[torch.arange(b, device=d()), target_indices, target]\n",
        "            log2probs = lnprobs / LOGE2\n",
        "            # -- The model produces natural logarithms of probabilities, but we need base-2 logarithms of the\n",
        "            #    probabilities, since these give us bits.\n",
        "\n",
        "            if tbw is not None:\n",
        "                for j, lp in enumerate(log2probs):\n",
        "                    i += 1\n",
        "                    tbw.add_scalar('compression/bits-per-token', -lp, i)\n",
        "\n",
        "                    if tok is not None:\n",
        "                        nc = len(tok.decode(target[j]))\n",
        "                        ic += nc\n",
        "                        tbw.add_scalar('compression/bits-per-byte', -lp/nc, ic)\n",
        "\n",
        "            bits += - log2probs.sum() # Add the bits for each character (the negative log_2 probabilities) to the running total\n",
        "            batch, target_indices = [], []  # clear the buffer\n",
        "\n",
        "    if isinstance(bits, torch.Tensor):\n",
        "        bits = bits.item()\n",
        "\n",
        "    return bits # total nr of bits used\n",
        "\n",
        "def estimate_compression(model, data, nsamples, context, batch_size, verbose=False):\n",
        "    \"\"\"\n",
        "    Estimates the compression by sampling random subsequences instead of predicting all characters.\n",
        "\n",
        "    NB: This doesn't work for GPT-2 style models with super-character tokenization, since the tokens and number of\n",
        "    characters are mismatched.\n",
        "\n",
        "    :param model: A sequence-to-sequence model that takes as input a (sub) sequence of integers and produces a probability\n",
        "    distributuion on the output.\n",
        "    :param data: A singe list of integers representing the  data\n",
        "    :return: The result of the computation in \"bits per byte\". That is, how many bits does the compressed representation\n",
        "    spend on each byte (=ASCII character) of the raw data.\n",
        "    \"\"\"\n",
        "\n",
        "    bits, tot = 0.0, 0\n",
        "    batch = []\n",
        "\n",
        "    # indices of target characters in the data\n",
        "    gtargets = random.sample(range(data.size(0)), k=nsamples)\n",
        "\n",
        "    # Buffer, every time it fills up, we run it through the model\n",
        "    # --- For the sake of speed we want to process the data in batches. For each token in the data, we make a\n",
        "    #     prediction based on all the `context` tokens before it. This means that for each subsequence in the batch, we\n",
        "    #     need to shift the start/end indices ahead by one token.\n",
        "    #\n",
        "    #     After we pass the batch through the model, we look at only the probabilities predicted for the last token.\n",
        "    target_indices = []\n",
        "    for current in tqdm.tqdm(gtargets) if verbose else range(gtargets):\n",
        "        # current is the character to be predicted\n",
        "\n",
        "        fr = max(0, current - context)\n",
        "        to = current + 1\n",
        "\n",
        "        instance = data[fr:to].to(torch.long) # the subsequence of the data to add to the batch\n",
        "        # -- slice out an instance of size context + 1 (or shorter at the start of the data)\n",
        "\n",
        "        target_indices.append(instance.size(0) - 2) # index of the last element of the context\n",
        "\n",
        "        if instance.size(0) < context + 1:\n",
        "            # the index in the output tensor of the character we want to predict\n",
        "            # -- It's context + 1, because we clip off the last token as a target\n",
        "\n",
        "            pad = torch.zeros(size=(context + 1 - instance.size(0),), dtype=torch.long)\n",
        "            instance = torch.cat([instance, pad], dim=0)\n",
        "            # -- the first tokens don't have enough tokens preceding them, so we pad them to the right size.\n",
        "\n",
        "            assert instance.size(0) == context + 1 # all instances should be `context` + 1 long\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            instance = instance.cuda()\n",
        "\n",
        "        batch.append(instance[None, :])\n",
        "        # -- We add a singleton dimension to concatenate along later.\n",
        "\n",
        "        if len(batch) == batch_size or current == data.size(0) - 1:\n",
        "            # batch is full or we are at the last instance, run it through the model\n",
        "\n",
        "            b = len(batch)\n",
        "\n",
        "            all = torch.cat(batch, dim=0)\n",
        "            inputs = all[:, :-1] # input\n",
        "            target = all[:, -1]  # target values\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if next(model.parameters()).is_cuda:\n",
        "                    inputs = inputs.cuda()\n",
        "                output = model(inputs)\n",
        "\n",
        "            if type(output) != torch.Tensor:\n",
        "                output = torch.log_softmax(output.logits, dim=2) # To make the method work for GPT2 models from Huggingface\n",
        "\n",
        "            assert output.size()[:2] == (b, context), f'was: {output.size()}, should be {(b, context, -1)}'\n",
        "\n",
        "            lnprobs = output[torch.arange(b, device=d()), target_indices, target]\n",
        "            log2probs = lnprobs * LOG2E\n",
        "            # -- The model produces natural logarithms of probabilities, but we need base-2 logarithms of the\n",
        "            #    probabilities, since these give us bits.\n",
        "\n",
        "            bits += - log2probs.sum() # Add the bits for each character (the negative log_2 probabilties) to the running total\n",
        "            batch, target_indices = [], []  # clear the buffer\n",
        "\n",
        "    return bits.item() # total nr of bits used"
      ],
      "metadata": {
        "id": "bPJgpKYygKv0"
      },
      "id": "bPJgpKYygKv0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##preprocessing / data augmentation"
      ],
      "metadata": {
        "id": "xqm-h4-orzrd"
      },
      "id": "xqm-h4-orzrd"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    xNorm = np.zeros_like(x)\n",
        "    #Get max and mins across all channels through trials and time bins\n",
        "    trainMaxofChannels = np.max(x, axis=(-1, -3))\n",
        "    print(trainMaxofChannels.shape)\n",
        "    trainMaxofChannels = trainMaxofChannels.reshape((22,1))\n",
        "    trainMinofChannels = np.min(x, axis=(-1, -3))\n",
        "    trainMinofChannels = trainMinofChannels.reshape((22,1))\n",
        "    minMaxofChannels = trainMaxofChannels - trainMinofChannels\n",
        "    # Use prevoius Values to calculate Min Max Normalization\n",
        "    # Normalizing across each trial\n",
        "    for i in range(x.shape[0]):\n",
        "        xNorm[i] = (x[i] - trainMinofChannels)/(trainMaxofChannels - trainMinofChannels)\n",
        "    return xNorm\n",
        "\n",
        "def standardize(x):\n",
        "    xStand = np.zeros_like(x)\n",
        "    #Get Mean and StDev across all channels through trials and time bins\n",
        "    trainChannelMean = np.mean(x, axis=(-1, -3))\n",
        "    trainChannelMean = trainChannelMean.reshape((22,1))\n",
        "    trainChannelStd = np.std(x, axis=(-1, -3))\n",
        "    trainChannelStd = trainChannelStd.reshape((22,1))\n",
        "    # Use prevoius Values to standardize\n",
        "    # Standardize across each trial\n",
        "    for i in range(x.shape[0]):\n",
        "        xStand[i] = (x[i] - trainChannelMean)/trainChannelStd\n",
        "    return xStand\n",
        "\n",
        "class FuncList:\n",
        "  def __init__(self, funcs):\n",
        "      \"\"\"\n",
        "      A list of functions to apply to an object.\n",
        "      :param funcs: a list of funcs.\n",
        "      \"\"\"\n",
        "      self.funcs = funcs\n",
        "\n",
        "  def apply(self, x):\n",
        "      for f in self.funcs:\n",
        "          x = f(x)\n",
        "      return x\n",
        "\n",
        "  def append(self, f):\n",
        "      self.funcs.append(f)\n",
        "\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group['lr']\n",
        "\n",
        "\n",
        "def trim_time(x, start_time, end_time) -> np.ndarray:\n",
        "    '''\n",
        "    Return trimmed subset of time-steps\n",
        "    '''\n",
        "    return x[:, start_time:end_time, :]\n",
        "\n",
        "\n",
        "def subsample(X, sub=5):\n",
        "  '''\n",
        "  Subsamples by averaging every adjacent *sub* samples provided by the parameter\n",
        "  Resulting length is datapoint length/sub\n",
        "  '''\n",
        "\n",
        "  time_length = X.shape[1]\n",
        "  if time_length % sub != 0:\n",
        "    raise Exception('Pick a sub that cleanly divises')\n",
        "\n",
        "  sub_shape = (X.shape[0], X.shape[1]//sub, X.shape[2])\n",
        "  out = np.zeros(sub_shape)\n",
        "\n",
        "  for dp in range(0, X.shape[0]): #loop through each datapoint\n",
        "    out_idx = 0\n",
        "    for ts in range(0, time_length, sub): #loop through each sub partition\n",
        "      out[dp][out_idx][:] = np.sum(X[dp][ts:ts+sub][:], axis=0) / sub\n",
        "      out_idx = out_idx + 1\n",
        "  \n",
        "  return out\n",
        "\n",
        "\n",
        "def duplicate_x(X, dup=2):\n",
        "  '''\n",
        "  Reduplicate a value dup times after it appears\n",
        "  Result length is length*dup\n",
        "  '''\n",
        "\n",
        "  time_length = X.shape[1]\n",
        "  print(time_length)\n",
        "\n",
        "  dup_shape = (X.shape[0], X.shape[1]*dup, X.shape[2])\n",
        "  out = np.zeros(dup_shape)\n",
        "\n",
        "  cur_out = 0\n",
        "  for dp in range(0, X.shape[0]): #loop through each datapoint\n",
        "\n",
        "    for ts in range(0, X.shape[1]): #loop thru each time slice in dp\n",
        "      org = X[dp][ts][:]\n",
        "\n",
        "      if cur_out >= X.shape[1]*dup:\n",
        "        break\n",
        "\n",
        "      for _ in range(0, dup): #loop dup amount of times to paste it in output\n",
        "        out[dp][cur_out][:] = org\n",
        "        cur_out = cur_out + 1\n",
        "      \n",
        "  \n",
        "  return out"
      ],
      "metadata": {
        "id": "jYXFAU1K4pAJ"
      },
      "id": "jYXFAU1K4pAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## self attention"
      ],
      "metadata": {
        "id": "DdceMqv1gOq0"
      },
      "id": "DdceMqv1gOq0"
    },
    {
      "cell_type": "code",
      "source": [
        "#Self Attention\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import random, math, sys\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, emb, heads=8, mask=False):\n",
        "    \"\"\"\n",
        "    :param emb:\n",
        "    :param heads:\n",
        "    :param mask:\n",
        "    \"\"\"\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    assert emb % heads == 0, f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'\n",
        "\n",
        "    self.emb = emb\n",
        "    self.heads = heads\n",
        "    self.mask = mask\n",
        "\n",
        "    s = emb // heads\n",
        "    # - We will break the embedding into `heads` chunks and feed each to a different attention head\n",
        "\n",
        "    self.tokeys    = nn.Linear(emb, emb, bias=False)\n",
        "    self.toqueries = nn.Linear(emb, emb, bias=False)\n",
        "    self.tovalues  = nn.Linear(emb, emb, bias=False)\n",
        "\n",
        "    self.unifyheads = nn.Linear(emb, emb)\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    b, t, e = x.size()\n",
        "    h = self.heads\n",
        "    assert e == self.emb, f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'\n",
        "\n",
        "    s = e // h\n",
        "\n",
        "    keys    = self.tokeys(x)\n",
        "    queries = self.toqueries(x)\n",
        "    values  = self.tovalues(x)\n",
        "\n",
        "    keys    = keys.view(b, t, h, s)\n",
        "    queries = queries.view(b, t, h, s)\n",
        "    values  = values.view(b, t, h, s)\n",
        "\n",
        "    # -- We first compute the k/q/v's on the whole embedding vectors, and then split into the different heads.\n",
        "    #    See the following video for an explanation: https://youtu.be/KmAISyVvE1Y\n",
        "\n",
        "    # Compute scaled dot-product self-attention\n",
        "\n",
        "    # - fold heads into the batch dimension\n",
        "    keys = keys.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "    queries = queries.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "    values = values.transpose(1, 2).contiguous().view(b * h, t, s)\n",
        "\n",
        "    queries = queries / (e ** (1/4))\n",
        "    keys    = keys / (e ** (1/4))\n",
        "    # - Instead of dividing the dot products by sqrt(e), we scale the keys and values.\n",
        "    #   This should be more memory efficient\n",
        "\n",
        "    # - get dot product of queries and keys, and scale\n",
        "    dot = torch.bmm(queries, keys.transpose(1, 2))\n",
        "\n",
        "    assert dot.size() == (b*h, t, t)\n",
        "\n",
        "    if self.mask: # mask out the upper half of the dot matrix, excluding the diagonal\n",
        "        mask_(dot, maskval=float('-inf'), mask_diagonal=False)\n",
        "\n",
        "    dot = F.softmax(dot, dim=2)\n",
        "    # - dot now has row-wise self-attention probabilities\n",
        "\n",
        "    # apply the self attention to the values\n",
        "    out = torch.bmm(dot, values).view(b, h, t, s)\n",
        "\n",
        "    # swap h, t back, unify heads\n",
        "    out = out.transpose(1, 2).contiguous().view(b, t, s * h)\n",
        "\n",
        "    return self.unifyheads(out)\n"
      ],
      "metadata": {
        "id": "kd_J5GmTFDnF"
      },
      "id": "kd_J5GmTFDnF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer block"
      ],
      "metadata": {
        "id": "QapkoNLCuio3"
      },
      "id": "QapkoNLCuio3"
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self, k, heads):\n",
        "    super().__init__()\n",
        "\n",
        "    self.attention = SelfAttention(k, heads=heads)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(k)\n",
        "    self.norm2 = nn.LayerNorm(k)\n",
        "    self.drop = nn.Dropout(p=0.1)\n",
        "\n",
        "    self.ff = nn.Sequential(\n",
        "      nn.Linear(k, k*4),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(p=0.1),\n",
        "      nn.Linear(k*4, k))\n",
        "\n",
        "  def forward(self, x):\n",
        "    attended = self.attention(x)\n",
        "    x = self.norm1(attended + x)\n",
        "    \n",
        "    fedforward = self.ff(x)\n",
        "    dp = self.drop(fedforward)\n",
        "    return self.norm2(dp + x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BasicBlockCNN(nn.Module):\n",
        "  def __init__(self, channels):\n",
        "      \"\"\"\n",
        "      A Basic conv block consisting of\n",
        "      skip ->\n",
        "      conv\n",
        "      bn\n",
        "      relu\n",
        "      conv\n",
        "      bn + skip <-\n",
        "      relu\n",
        "      :param channels: the number of channels for this basic block\n",
        "      \"\"\"\n",
        "      super(BasicBlockCNN, self).__init__()\n",
        "      self.conv1 = nn.Conv1d(channels, channels, 3, padding=1)\n",
        "      self.bn1 = nn.BatchNorm1d(channels)\n",
        "      self.conv2 = nn.Conv1d(channels, channels, 3, padding=1)\n",
        "      self.bn2 = nn.BatchNorm1d(channels)\n",
        "      self.drop = nn.Dropout(p=0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = F.relu(out)\n",
        "      \n",
        "      out = self.drop(out)\n",
        "      \n",
        "      out = self.conv2(out)\n",
        "      out = self.bn2(out) + x\n",
        "      out = F.relu(out)\n",
        "      return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class DeepCNNBase(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeepCNNBase, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels = 22, out_channels = 25,kernel_size=(10,1),padding = 'same')\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=(3,1),stride = 3)\n",
        "        self.conv2 = nn.Conv2d(in_channels = 25,out_channels = 50,kernel_size = (10,1),padding = 'same')\n",
        "        self.conv3 = nn.Conv2d(in_channels = 50,out_channels = 100,kernel_size = (10,1),padding = 'same')\n",
        "        self.conv4 = nn.Conv2d(in_channels = 100,out_channels = 200,kernel_size = (10,1),padding = 'same')\n",
        "        self.gru = nn.GRU(input_size = 12, hidden_size = 64, num_layers=1, batch_first = True)\n",
        "        self.bn1 = nn.BatchNorm2d(25)\n",
        "        self.bn2 = nn.BatchNorm2d(50)\n",
        "        self.bn3 = nn.BatchNorm2d(100)\n",
        "        self.bn4 = nn.BatchNorm2d(200)\n",
        "        self.dropout = nn.Dropout(p=0.75)\n",
        "        self.ELU = nn.ELU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear1 = nn.Linear(12800,4)\n",
        "        #self.linear1 = nn.Linear(2560,4)\n",
        "\n",
        "\n",
        "        #transformer\n",
        "        tblocks = []\n",
        "        t_depth = 2\n",
        "        t_heads = 1\n",
        "        t_dim = 12\n",
        "        t_seq_length = 1000\n",
        "        \n",
        "        self.pos_emb = nn.Embedding(t_seq_len, t_dim)\n",
        "\n",
        "        for i in range(t_depth):\n",
        "            tblocks.append(TransformerBlock(k=t_dim, heads=t_heads))\n",
        "        self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "\n",
        "        #transformer block\n",
        "        '''\n",
        "        b, t, k = x.size() #b, t, k = tokens.size().    64, 200, 64 :: 64, 1000, 22 (b, seq_length, channels)\n",
        "\n",
        "        # generate position embeddings\n",
        "        positions = torch.arange(t)\n",
        "        positions = positions.to('cuda:0')\n",
        "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
        "          \n",
        "        x = x + positions\n",
        "        x = self.tblocks(x)\n",
        "        '''\n",
        "\n",
        "        \n",
        "        x = np.swapaxes(x, 1, 2)\n",
        "\n",
        "        x = torch.reshape(x,(x.shape[0],x.shape[1],x.shape[2],1))\n",
        "\n",
        "        #print(x.shape)\n",
        "\n",
        "        \n",
        "\n",
        "        ## Conv Pool Block 1\n",
        "        x = self.conv1(x)\n",
        "        x = self.ELU(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        ## Conv Pool Block 2\n",
        "        x = self.conv2(x)\n",
        "        x = self.ELU(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.dropout(x)\n",
        "               \n",
        "        ## Conv Pool Block 3\n",
        "        x = self.conv3(x)\n",
        "        x = self.ELU(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        ## Conv Pool Block 4\n",
        "        x = self.conv4(x)\n",
        "        x = self.ELU(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x = np.squeeze(x)\n",
        "        device = x.device\n",
        "        \n",
        "        # print(x.shape)\n",
        "        M = x.shape[0]\n",
        "\n",
        "        #h0 = torch.zeros(self.gru.num_layers, M, self.gru.hidden_size, requires_grad=True).to(device)\n",
        "        #x, _ = self.gru(x, h0)\n",
        "\n",
        "\n",
        "        #print('ONE:', x.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #'''Transformerrrr\n",
        "        #x = np.swapaxes(x, 1, 2)\n",
        "\n",
        "        #transformer block\n",
        "        b, t, k = x.size() #b, t, k = tokens.size().    64, 200, 64 :: 64, 1000, 22 (b, seq_length, channels)\n",
        "\n",
        "        # generate position embeddings\n",
        "        positions = torch.arange(t)\n",
        "        positions = positions.to('cuda:0')\n",
        "        positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
        "          \n",
        "        x = x + positions\n",
        "        x = self.tblocks(x)\n",
        "\n",
        "        #'''\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        #print('TWO:', x.shape)\n",
        "        #assert(False)\n",
        "\n",
        "\n",
        "        #Flatten\n",
        "        x = self.flatten(x)\n",
        "        #print(\"flatten output: \", x.shape)\n",
        "        x = self.linear1(x)\n",
        "\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "rrK1ahoOu0Qj"
      },
      "id": "rrK1ahoOu0Qj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer"
      ],
      "metadata": {
        "id": "AEVaNi6Tu0mA"
      },
      "id": "AEVaNi6Tu0mA"
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
        "    super().__init__()\n",
        "\n",
        "    self.num_tokens = num_tokens\n",
        "    #self.token_emb = nn.Embedding(num_tokens, k)\n",
        "    self.pos_emb = nn.Embedding(seq_length, k)\n",
        "\n",
        "    self.drop = nn.Dropout(p=0.65)\n",
        "\n",
        "\n",
        "    #cnnc stuff\n",
        "    self.conv1 = nn.Conv1d(22, 22, 5, stride=1)  # (B, 22, 1000) -> (B, 22, 200)\n",
        "    self.b1 = BasicBlockCNN(22)  # (B, 22, 200) -> (B, 22, 200)\n",
        "    self.dd = DeepCNN()\n",
        "\n",
        "    # The sequence of transformer blocks that does all the \n",
        "    # heavy lifting\n",
        "    tblocks = []\n",
        "    for i in range(depth):\n",
        "        tblocks.append(TransformerBlock(k=k, heads=heads))\n",
        "    self.tblocks = nn.Sequential(*tblocks)\n",
        "\n",
        "    # Maps the final output sequence to class logits\n",
        "    self.toprobs = nn.Linear(k, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "    :param x: A (b, t) tensor of integer values representing \n",
        "              words (in some predetermined vocabulary).\n",
        "    :return: A (b, c) tensor of log-probabilities over the \n",
        "              classes (where c is the nr. of classes).\n",
        "    \"\"\"\n",
        "    # generate token embeddings\n",
        "    #tokens = self.token_emb(x)\n",
        "\n",
        "    \n",
        "   \n",
        "\n",
        "    #x = self.drop(x)\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    #print(\"X SIZE BEFORE:\", x.size())\n",
        "\n",
        "    x = np.swapaxes(x, 1, 2)\n",
        "    #print(\"X SIZE BEFORE SWAPPED:\", x.size())\n",
        "    #CNN layer module thing\n",
        "    #x = self.conv1(x)\n",
        "\n",
        "\n",
        "    x = self.dd(x)\n",
        "    \n",
        "\n",
        "    #print(\"X SIZE AFTER:\", x.size())\n",
        "    #x = np.swapaxes(x, 1, 2)\n",
        "    x = self.drop(x)\n",
        "    \n",
        "    \n",
        "\n",
        "    #x = np.swapaxes(x, 1, 2)\n",
        "    x = self.b1(x)\n",
        "    x = np.swapaxes(x, 1, 2)\n",
        "\n",
        "\n",
        "    b, t, k = x.size() #b, t, k = tokens.size()\n",
        "\n",
        "    # generate position embeddings\n",
        "    positions = torch.arange(t)\n",
        "    positions = positions.to('cuda:0')\n",
        "    positions = self.pos_emb(positions)[None, :, :].expand(b, t, k)\n",
        "      \n",
        "    x = x + positions\n",
        "    x = self.tblocks(x)\n",
        "\n",
        "    \n",
        "    # Average-pool over the t dimension and project to class \n",
        "    # probabilities\n",
        "    \n",
        "    x = self.toprobs(x.mean(dim=1))\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "1JG8cUVSvuai"
      },
      "id": "1JG8cUVSvuai",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training loops (our way)"
      ],
      "metadata": {
        "id": "bxr2zYnaK6uw"
      },
      "id": "bxr2zYnaK6uw"
    },
    {
      "cell_type": "code",
      "source": [
        "#Random stuff we need\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!git clone https://github.com/TaykhoomDalal/ECE_C147_Project.git\n",
        "%cd 'ECE_C147_Project'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fo7DpHpLGwI",
        "outputId": "47e2390b-a054-475b-890f-05bc1d4013a7"
      },
      "id": "0fo7DpHpLGwI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path 'ECE_C147_Project' already exists and is not an empty directory.\n",
            "/content/ECE_C147_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# TRAIN MODEL FUNCTION (train_model.py)\n",
        "#\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from utils import parse_args_with_config, load_data\n",
        "from datasets import NpDataset\n",
        "import models\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "import os\n",
        "from torchvision import transforms\n",
        "from utils import load_data\n",
        "from torch.utils.data import DataLoader\n",
        "from train import train, validate\n",
        "\n",
        "\n",
        "#ARGS\n",
        "args = {'gpu': 0, \n",
        "        \n",
        "        'dataset_root': '../drive/MyDrive/W2022/ECE C247/Final Proj stuff/data',\n",
        "\n",
        "        'epochs': 1500,\n",
        "        'batch_size': 64,\n",
        "        'lr': 0.01,\n",
        "        'lr_milestones': [350, 750, 1250],\n",
        "        'lr_gamma':0.12,\n",
        "\n",
        "        'num_workers': 2,\n",
        "        'pin_memory': True,\n",
        "        'channels_first': True,\n",
        "        \n",
        "        'run_name': 'template', \n",
        "        'log_root': 'logs/'}\n",
        "\n",
        "#cuda\n",
        "if torch.cuda.is_available():\n",
        "  device = f'cuda:{args[\"gpu\"]}'\n",
        "else:\n",
        "  device = None\n",
        "\n",
        "t_heads = 2\n",
        "t_depth = 1\n",
        "t_seq_len = 100\n",
        "\n",
        "#model = Transformer(k=22, heads=t_heads, depth=t_depth, seq_length=t_seq_len, num_tokens=100, num_classes=4)\n",
        "model = DeepCNNBase()\n",
        "model = model.to(device)\n",
        "#High Val: 46.7\n",
        "weights_save_file_name = 'transformer_h' + str(t_heads) + '_d' + str(t_depth) + '_l' + str(t_seq_len) + '.pt'\n",
        "\n",
        "\n",
        "\n",
        "# logging\n",
        "log_name = args['run_name'] + \"_\" + str(int(time.time()))\n",
        "writer = SummaryWriter(os.path.join(args['log_root'], log_name))\n",
        "\n",
        "# transforms and online data augmentation\n",
        "transform_train = None\n",
        "transform_test = None\n",
        "\n",
        "\n",
        "\n",
        "# load dataset\n",
        "#'''GUARD\n",
        "data = load_data(args['dataset_root'])\n",
        "\n",
        "# data is channels last by default (n, l, c)\n",
        "# conv nets need channels first ie (n, c, l)\n",
        "# optionally flip channels here\n",
        "if args['channels_first']:\n",
        "    data['X_train_valid'] = np.transpose(data['X_train_valid'], axes=(0, 2, 1))\n",
        "    data['X_test'] = np.transpose(data['X_test'], axes=(0, 2, 1))\n",
        "\n",
        "#GUARD'''\n",
        "\n",
        "\n",
        "\n",
        "#PREPROCESSING\n",
        "init_instances = len(data['X_train_valid'])\n",
        "\n",
        "#Trimming (first 500)\n",
        "'''\n",
        "trimmed = trim_time(data['X_train_valid'], 500, 1000)\n",
        "trimmed = duplicate_x(trimmed)\n",
        "data['X_train_valid'] = np.append(data['X_train_valid'], trimmed, axis=0)\n",
        "data['y_train_valid'] = np.append(data['y_train_valid'], data['y_train_valid'])\n",
        "'''\n",
        "\n",
        "#subsampling (sub 25)\n",
        "'''\n",
        "data['X_train_valid'] = subsample(data['X_train_valid'], sub=1000//t_seq_len)\n",
        "data['X_test'] = subsample(data['X_test'], sub=1000//t_seq_len)\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#add noise\n",
        "'''GUARD\n",
        "num_instances = len(data['X_train_valid'])\n",
        "for instance in range(num_instances):\n",
        "  noise = np.random.normal(0, .1, (1, t_seq_len, 22))\n",
        "  new_signal = data['X_train_valid'][instance] + noise\n",
        "  data['X_train_valid'] = np.append(data['X_train_valid'], new_signal, axis=0)\n",
        "  data['y_train_valid'] = np.append(data['y_train_valid'], data['y_train_valid'][instance])\n",
        "'''\n",
        "\n",
        "\n",
        "# create target to index mapping\n",
        "unique_targets = np.unique(data['y_train_valid'])\n",
        "offset = np.min(unique_targets)\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = NpDataset(data['X_train_valid'], data['y_train_valid'] - offset, transform=transform_train, store_as_tensor=True)\n",
        "test_dataset = NpDataset(data['X_test'], data['y_test'] - offset, transform=transform_test, store_as_tensor=True)\n",
        "\n",
        "# dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=args['batch_size'], shuffle=True, num_workers=args['num_workers'], pin_memory=args['pin_memory'])\n",
        "test_loader = DataLoader(test_dataset, batch_size=args['batch_size'], num_workers=args['num_workers'], pin_memory=args['pin_memory'])\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "#load onto gpu\n",
        "for inputs, labels in train_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "for inputs, labels in test_loader:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "\n",
        "#GUARD'''\n",
        "# criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "'''\n",
        "# lr scheduler\n",
        "#lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, args['lr_milestones'], args['lr_gamma'])\n",
        "\n",
        "'''\n",
        "best_val_acc = 0\n",
        "best_model = None\n"
      ],
      "metadata": {
        "id": "-C21nAu7LAdY"
      },
      "id": "-C21nAu7LAdY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['X_train_valid'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3f-vVPIs2vG",
        "outputId": "cc42ee32-5b92-4808-da4a-c76aa5276e61"
      },
      "id": "Z3f-vVPIs2vG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4230, 1000, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(\n",
        "    [\n",
        "      {\"params\": model.tblocks.parameters(), \"lr\": 0.4},\n",
        "     {\"params\": model.conv1.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.maxpool1.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.conv2.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.conv3.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.conv4.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.gru.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.bn1.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.bn2.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.bn3.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.bn4.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.ELU.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.flatten.parameters(), \"lr\": 0.1},\n",
        "     {\"params\": model.linear1.parameters(), \"lr\": 0.1},\n",
        "    ], \n",
        "    lr=args['lr']\n",
        ")"
      ],
      "metadata": {
        "id": "RjnU7mUQzpya"
      },
      "id": "RjnU7mUQzpya",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################################################################################################################################\n",
        "#Edit Learning Rate\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "PDCEwQSmAkwi"
      },
      "id": "PDCEwQSmAkwi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Assign best Model\n",
        "model = best_model"
      ],
      "metadata": {
        "id": "e6KNy2ZfScFS"
      },
      "id": "e6KNy2ZfScFS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load .pt model from Drive\n",
        "t_heads = 2\n",
        "t_depth = 1\n",
        "t_seq_len = 25\n",
        "\n",
        "model = Transformer(k=22, heads=t_heads, depth=t_depth, seq_length=t_seq_len, num_tokens=100, num_classes=4)\n",
        "ref_path = '../drive/MyDrive/W2022/ECE C247/Final Proj stuff/transformers/checkpoints/'\n",
        "file_path = ref_path + 'transformer_h' + str(t_heads) + '_d' + str(t_depth) + '_l' + str(t_seq_len) + '.pt'\n",
        "model.load_state_dict(torch.load(file_path))\n",
        "model = model.to('cuda:0')\n",
        "#################################################################################################################################"
      ],
      "metadata": {
        "id": "nMrlvcUVpxEJ"
      },
      "id": "nMrlvcUVpxEJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train loop stats\n",
        "for e in range(args['epochs']):\n",
        "    print(\"Best Val:\", best_val_acc)\n",
        "\n",
        "    # train\n",
        "    model.train()\n",
        "    train_loss, train_acc = train(model, criterion, optimizer, train_loader, e, device=\"cuda:0\")\n",
        "\n",
        "    # validate\n",
        "    model.eval()\n",
        "    val_loss, val_acc = validate(model, criterion, test_loader, e, device=device)\n",
        "\n",
        "    print(\"\")\n",
        "\n",
        "\n",
        "    if val_acc >= best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      best_model = model\n",
        "      file_name = '../drive/MyDrive/W2022/ECE C247/Final Proj stuff/transformers/checkpoints/' + weights_save_file_name\n",
        "      torch.save(model.state_dict(), file_name)\n",
        "\n",
        "\n",
        "    # update learning rate\n",
        "    #lr_scheduler.step()\n",
        "\n",
        "\n",
        "    if e % 100 == 0:\n",
        "      print(\"\\n\\n\\nLR: \")\n",
        "      print(get_lr(optimizer))\n",
        "\n",
        "    '''\n",
        "    # log stats\n",
        "    best_val_acc = max(val_acc, best_val_acc)\n",
        "    writer.add_scalar(\"loss/train\", train_loss, e)\n",
        "    writer.add_scalar(\"acc/train\", train_acc, e)\n",
        "    writer.add_scalar(\"loss/val\", val_loss, e)\n",
        "    writer.add_scalar(\"acc/val\", val_acc, e)\n",
        "    writer.add_scalar(\"acc/val_best\", best_val_acc, e)\n",
        "    writer.add_scalar(\"optim/lr\", lr_scheduler.get_last_lr()[0], e)\n",
        "    '''\n",
        "\n",
        "# log hyperparams\n",
        "'''\n",
        "writer.add_hparams({\n",
        "    \"model\": args.model,\n",
        "    \"optimizer\": args.optimizer,\n",
        "    \"batch_size\": args.batch_size,\n",
        "    \"learning_rate\": args.learning_rate,\n",
        "    \"momentum\": args.momentum,\n",
        "    \"l2_reg\": args.l2_reg,\n",
        "    \"epochs\": args.epochs\n",
        "}, {\n",
        "    \"best_val_acc\": best_val_acc\n",
        "})\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "8VG9hQUajOOL",
        "outputId": "0472168f-10a6-4ff6-a69b-56ea43c0735b"
      },
      "id": "8VG9hQUajOOL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Val: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train 0:   0%|          | 0/34 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:443: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n",
            "Train 0:   0%|          | 0/34 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5227abe28c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# validate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ECE_C147_Project/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, optimizer, train_loader, epoch, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-760eec8ef910>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m#'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-760eec8ef910>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mattended\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattended\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-9ed574a80c77>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mkeys\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoqueries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mvalues\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtovalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
          ]
        }
      ]
    }
  ]
}